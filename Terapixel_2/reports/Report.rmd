---
title: "Report"
author: "Callum Simpson"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```


```{r include=FALSE}
options(digits=2)

setwd("D:/Masters/Cloud/Terapixel/Terapixel_v2/Terapixel_2")
library('ProjectTemplate')
load.project()

source('src/Events_Explore_code.R')
source("src/GPU_Power_Usage_code.R")
source("src/GPU_heat_code.R")
source("src/GPU_Power_Usage_code.R")
source("src/render_map.R")

```

```{r test}
getwd()
```

This is my report of my CSC8634: Cloud computing with Project.

In this report, I will be discussing how I undertook a Performance evaluation project on a set of data relating to the creation of a Terapixel image rendered in Cloud (Super)computing.

## A background 

A Terapixel image offers a new and intuitive way to present information sets to stakeholders that is also extremely accessible to all, allowing viewers to interactively browse big data across multiple scales. Typically they're made up of over one trillion pixels and provide a fluid experience where the viewer can see an overview of the whole image or zoom into incredible detail. 

The most important aspect of a Terapixel image is that viewing it only depends on the image display capabilities of the web browser used, making Terapixel images accessible for a wide range of clients as no expensive software or hardware is needed to view the image, only a web browser and a link. Terapixels can also be very detailed allowing for impressive designs that a client might require.

This project will be a performance evaluation looking at processes behind the creation of the supposed first terapixel visualization of IoT data within a 3D urban environment created by Members of Newcastle university in the report Petascale Cloud Super computing for Terapixel Visualization of a Digital Twin.

The image
http://terapixel.wasabi.1024.s3.eu-central-1.wasabisys.com/vtour/index.html

The image created was a realistic 3D visualization of the city of Newcastle upon Tyne to display psychical locations of a set of sensors that are located around Newcastle University. The creation of this Terapixel image was done by using a scalable architecture for cloud-based visualization. This means a user can control what they deploy and pay for.

The project had three key goals 
- create a supercomputer architecture for scalable visualization using the public cloud
- produce a Terapixel 3D city visualization supporting daily updates 
- undertake a rigorous evaluation of cloud supercomputing for compute-intensive visualization applications.

This Terapixel was visualized using a path tracing rendered and 1024 public IaaS cloud GPU nodes, creating the image in under an hour. The image is rendered over 65,793 tiles (with each tiles being a square from the image). The image is split up into all the rendering tasks and then the N up to 1024 render tiles in parallel. Once each render tile is stitched back together and stored on one Tbyte of Azure Blob storage. 

Iaas (Infrastructure as a Service) if a form of cloud computing that provides virtualized computing resources over the internet from a third-party provider to create an IT environment that best fits business requirements. The benefit of IaaS is that they significantly reduce the cost of IT infrastructure because it allows the use of access to cloud resources without having to purchase a data center hardware/maintain equipment.

The responsibility of maintaining the infrastructure falls upon the IaaS provider whose job it is to ensure its availability. 

## What is the reasons for this project

This project aims to undertake a Performance evaluation on the creation of this TeraPixel image using cloud computing via EDA to find keys areas in which improvements could be made. We have data relating to the different tasks needed to create the image, recordings of GPU usage and the coordinates of each tile (so what part of the map is being worked on).

The two main areas that we can focus on in this project are finding ways that could speed up the creation of the image and finding issues with the GPUS used.

The Terapixels two main costs is the time needed to generate the image and Money. As IaaS resources are rented, if we can find ways to reduce the amount of time needed to Generate a Terapixel image we would be able to rent servers for less time (therefore saving money). Also, decreasing the time needed to generate an image would mean that more images could be made within a given time frame, which would be beneficial for clients. 

We see that the creation of the Terapixel image was created in 43 minutes. It also requires the rendering of 65,793 tiles (image is made up of 12 layers and levels 4,8,12 need to be completely render as they are used to fill out the other lays).

Secondly, As we have the data about the clouds 1024 nodes that were used to create this image we can perform an analysis on the cloud side of the project to see how the processes affected the GPUS that were used. Hardware issues could slow down the speed at which the image is created or lead to other problems. If we can highlight potential issues then we could fix/replace the GPUS, hopefully resulting in future Terapixel running more smoothly.

## My response 

For this project, I performed Exploratory Data Analysis to discover patterns, spot anomalies and check assumptions via summary statistics and graphical representations.

There where many questions I could ask and try to answer with this data. Because of this, I decided that I would follow a Crisp-DM model while undertaking my analysis. From the start, I knew some of the questions I wanted to ask and I fully expected that what I may discover new information that may help me gain a better understanding of a question that I previously asked and could possibly suggest a new question/route of analysis that I should go explore. The crisp DM model would be perfect for this line of questioning as it clearly gave me steps that I could follow in order to ensure that I wouldn't get confused or lost in my analysis. 

Business understanding was getting a better understanding of the report created with this project. My business goals were finding ways to reduce the time need to create an image and find a possible issue with GPUs.  My analysis would be complete if I could find evidence that could be used to improve either of these.

Understanding of Data was me getting grips with the data I had been given.

Data preparation was me getting the data clean and ready to answer the question.
 
Modeling was the graph or status that I got back. If I felt I needed more to answer the question I went back to data preparation to change the data to see if it would answer my question.

The evaluation had me checking to see if the answer to the question asked helped me solve my business goals. If I felt like it didn't I went back through the cycle with the newly gained knowledge to see if I could apply it somewhere to see if that would help me achieve my goals.

Deployment is this report.

Crisp DM has been used many in many different projects with great success.

When looking into the cost of power and time how they effected things like heat and utilization I used a mixture of univariate analysis to analyses each feature and Bivariate analysis to work out the how some elements affected another element (primarily how X effects time).

When looking into The GPUS to try and find any issues I mainly performed univariate analysis to try and discover the medians that each GPU produced, That way I could look for the GPUS that had a higher median than the grouped average. For example, if I worked out the median power usage for all GPUs and found that most had a median of 40 but one had a median of 50 then we know something might be up with that GPU.

### Reports in which I used for insperation

#### Petascale Cloud Supercomputing for Terapixel Visualization of a Digital Twin 
by Nicolas S. Holliman, Member IEEE Computer Society, Manu Antony,James Charlton, Stephen Dowsland, Philip James and Mark Turner. 

This is the report about the creation of Terapixel image. This was used to get a better understanding of what I was looking at and why it was made.

link https://arxiv.org/ftp/arxiv/papers/1902/1902.04820.pdf


#### Cloud service performance evaluation: status, challenges, and opportunitiesâ€“a survey from the system modeling perspective 
By Qiang Duan. Discuss Typical 

metrics used for evaluating cloud service performance and the challenges that come from them. Gave me some inspiration on to how I might undertake some evaluation on the GPUS. For example using Service response time to see if there where any GPUS that took longer than others.

link https://www.sciencedirect.com/science/article/pii/S2352864816301456


#### Butterfly Image Classification Using Color Quantization Method on HSV Color Space and Local Binary Pattern 
by Dhian Satria Yudha Kartika, Darlis Heru Murti, Anny Yuniarti

Halfway through my project I wanted to work out how you could abstract what is in an image in order to classify it. Whilst investigating I discovered this paper. It taught me about Color Quantization and how I could use it to help me solve my problem. 

link https://core.ac.uk/download/pdf/295411033.pdf


#### KEY METRICS OF VIDEO CARDS 
by xoticpc.com

I found it difficult finding methods on how you measure the performance of a GPU. This is best I could find. It discuss a few methods but I think given the data the only one I can use is Pixel fill rates (how many pixels a GPU can process in a second) .

Exploratory Data Analysis [EDA]: Techniques, Best Practices and Popular Applications - By Simplilearn

I wanted to get a better Idea of some the tools and techniques behind EDA so I knew what tools I could use before I started.

https://www.simplilearn.com/exploratory-data-analysis-article


#### Exploratory data analysis 
by Wikipedia

Helped me get a better understanding of some of graphs that I should be using in this project and how they work.

https://en.wikipedia.org/wiki/Exploratory_data_analysis


## What I did and what I discovered.

For this project, I mainly performed Exploratory Data Analysis to discover patterns, to spot anomalies and to check assumptions via summary statistics and graphical representations.

The data that I received where 

- application-checkpoints : The application checkpoint events throughout the execution of the render job. Data in this was primarily Polynomial Variable (i.e events and tasks names) with the Continuous Variable of time.

- gpu :  metrics that were output regarding the status of the GPU on the virtual machine. Key data in this was Continuous (i.e amount of power used)

- task-x-y : the x,y coordinates of which part the image was being rendered for each task. Mainly made up of Discrete Variables.

Before I really started anything I went through each variable in each data set to really understand what I was working with. Through checks, I discovered that there didn't seem to be any issues with any of the data (apart from a few duplication) and it was really clean.

The following will be some of my analysis into the questions that I asked and what I discovered. The order in the questions appear in is the order I went through them in my Crisp-dm cycle however I did go back and update past sections If I found some new information that was useful.

A TLDR would be, After discovering that rendering event was the part of each task I that took the most time to complete I decided looked over how rendering effected heat and power to discover that an increase in rendering time caused an increase in both of them. This lead me to creating a K means system called colour Quantization to work out what was in each tile to see what was being rendered had any effect on the rendering task. After that some investigation was done to see if I could find errorus GPUS.

### Which event types dominate task runtimes

For more detail for the section Please see "Events_Explore" report.

The First question I asked was "Which event types dominate task runtimes". I felt that this was a good first question to answer as if I want to reduce the amount of time needed to create an image then seeing what event took the most amount of time it would tell me what I needed to investigate. After working out the time that had passed between an event starting and ending I worked out the median for each and then worked out the percentage of how much of the total task time was spent on that event.

```{r time_percentage}
ggplot(data=time_sumary_omit_totalrender %>% arrange((percenatge)) )+
  geom_bar(aes(x="", y=percenatge, fill=eventName), stat="identity", width = 1)+
  coord_polar("y", start=0)+
  theme_void() + ggtitle("On average, what percentage of task run time did each event take up" ) + scale_fill_discrete(name="Event name") 

```

Using the Pie chart we see that Rendering is the dominating event (TotalRender was removed due to it being the combination of all other events combined) and none of the other events came close. A further check on if different jobs had any effect on events percentage make up of total time I found out that didn't par from uploading events in job 8 making up a higher percentage of run time. 

This told me that if we want to speed up the creation of the image we would need to reduce the amount of time spent rendering. Also, the higher median uploading in job 8 suggests that something had gone wrong in uploading in this job.

Whilst investigating this I discovered that the media time for a task to complete is about 43 seconds but that the typical time taken to complete a full task does change based on the job.

```{r last_task_boxplot}
last_task_boxplot
```
Looking at the box plot of the amount of time it took to complete each task we see that we have a few outliers. A few of these ouliers have times that are almost double the median, this is an issue that we will need to look into this. 

By combining the data frame which has the time taken to complete a task mixed with the x and y cords we could use it to visualize a heat map of xy cords and how long it took to render that tile. 

```{r last_tasks}

last_tasks_xy <- left_join(last_tasks, xy_df ,by = c("jobId","taskId") )

last_tasks_xy <- last_tasks_xy %>% mutate(timestamp = as.POSIXct(timestamp,format="%Y-%m-%d %H:%M:%OS"), time_from_start = as.numeric(timestamp - as.POSIXct("2018-11-08 07:41:55.289",format="%Y-%m-%d %H:%M:%OS")))

ggplot(last_tasks_xy, aes(y, x, fill = total_time)) + geom_tile()+ 
  facet_wrap(~ level,scales = "free")+   scale_fill_gradient(low="grey", high="blue") + ggtitle("Time needed to complete each tile by each level") + scale_y_reverse()  + theme(legend.position="bottom") + labs(fill = "Time taken (seconds)")
```

Looking at the lvl 8 and 12 graphs we see that there is are distinct areas in which the render time was really low (top left). This is a flat area that suggest that what is in the tile that is being rendered does seem to effect the time taken to complete the task.

We also notice that there seems to be a distinct band in the low Y cords in the job lvl 12 graph in which rows of x cords seem to have the same or similar high completion times. We also see a similar band appear in the y = 10 ish in lvl 8. This seems really off and was the subject of later investigation.

### What is the interplay between GPU temperature and performance

For a more detailed overview please look at the report "GPU_heat".

Next, I wanted to look into GPU temperature and performance. The high temperature will have two negative effects on a GPU.

- A high temperatures will shorten the life of hardware by damaging the hardware
- GPU typically have a fail-safe that will try and combat high temperature by ether causing a slow down to cool off or fully shut down. Both would increase the time needed to run a program as it essentially takes out one of the GPUs for a bit.

The median heat per tick is 40 degrees and that the data is normally distributed.

First I wanted to see how heat changes the memory utlisation. The error bars are there to show spread the data are around the meidan value

```{r Temp_memory}
Temp_memory + labs(caption = "Error bars are spread around point") + xlab("GPU Memory utilisation percentage")
```

We see that there is as Memory utilization increases it changes the temperature in some weird ways. 

At low utilization zero (so not running) temperature as its lowest of 37 Celsius. we then see as utilization slightly increases we see a a jump to around 40 degrees. We then notice 3 trends
- between 5% and 40% memory utilization the temperature stays at around 40 degrees. 
- There is then a sudden jump to around 41 degrees up until 60% utilization
- Between memory utilization 60% to 80% we see that temperature slowly starts to decrease to around 38 degrees.

We notice that the higher gpu memory utilization the median gets a bit weird, this is due to us having less records of higher GPU memory utilization at high percentages. 

I also wanted to see how GPU utilization effected temperature.

```{r Temp_gpu}
Temp_gpu  + labs(caption = "Error bars are spread around point") + xlab("GPU utilisation percentage")
```

We see that there is a slow increase in temperature as GPU utilization increases. This is what we would expect, the more a device is utilized then the more "pieces" are moving causing an increase in temperature. 

One statistic that can be used to calculated the performance of a GPU is fill rate, the number of pixels that can be rasterize by the card and write to memory per second. We know that each task is basically the filling in of 4096 * 4096 pixels so we can say that tasks with low total rendertimes could be said to have a better performance. On the flip side of that we can say that task that took longer to render had a worse performance.

By selecting the rendering events in checkpoints and linking via host name and time to the appropriate GPU in the GPU data frame I was able to create a summary of power usage depending on the length on the rendering processes.

Calculating the correlation between GPU variables and render time  ...

```{r Correlation}

ggcorrplot(cor(gpu_render_highlight  %>% rename(powerDrawWatt = powerDrawWatt.x, gpuTempC = gpuTempC.x,  gpuUtilPerc = gpuUtilPerc.x , gpuMemUtilPerc = gpuMemUtilPerc.x ) %>% filter(render == 1) %>% select(powerDrawWatt, gpuTempC, gpuUtilPerc, gpuMemUtilPerc,render_time))
, hc.order = TRUE, type = "lower",
     outline.col = "white", lab = TRUE) + ggtitle("Correlation between GPU variables ") + labs(fill = "Correlation")

```

We see that there is a slight correlation between render time and gpu temptaure (0.14) meaning that an increase in render time does have an effect (though not major) on GPU heat. We also see that as GPU utilization increase that temperature increases (0.42).

This suggest that If we can work out a way to reduce render time we would also be able to reduce the amount of heat produced per GPU. If we aren't able to find ways to reduce render times than a new cooling system may need to be put in place.

### What is the interplay between increased power draw and render time

For a more detailed breakdown please see GPU_Power_Usage report 

Through my investigation I found that the median gpu power used each secound is `r sum(gpu_df$powerDrawWatt) / (50 * 60)` watts.

Using the application-checkpoints data frame to see what time rendering took place and combined it to the gpu df by host name I was able to work out which ticks in the GPU data frame where happing during a task render event and which where happening when another event was occurring.

A quick view over how power usage changed in rendering and none rendering events in one of the GPUS we saw that rendering a tasks started off at lower power, spiked up to high power then near the end dropped back to lower power. We also saw that the longer the render time the longer and higher this high power line reached.

```{r single_host_plot}
single_host_plot 

```

This gave me the idea to work out how far through the rendering task each tick was and assign it a percentage.

Using the medians power usage in watts for all the percentages render task completion we can get the following.

```{r Power_used_at_all_percentage}

ggplot(Power_used_at, aes(rendering_percenatge,median)) + geom_line()  + geom_point() + geom_errorbar(aes(ymin=qt1, ymax=qt3), width=0.25) +  ggtitle("Medain power used at each render completion percent")+ ylab("Power usage in Watts") +  xlab("Precentage completion of Render task")  + labs(caption = "Error bars are spread around point")  

```

We see that typically the first 20ish percent of the rendering processes the power usage is around 30 watts per tick. Once we reach 20% percent of the way through the rendering processes power usage dramatically jumps to around 110 watts per tick (A tick being every 2 seconds). We see that power usage continues to stay around 110 watts until we reach roughly 95% when power watts jump back down to roughly 45 watts per tick. This tells us that during the rendering processes it takes a few moments before the processes to actually start rendering and once it's done there is a few seconds where the render "cools off".

By plotting out the amount of time needed to render a tile and plotting each as a line graph of power used by percentage completion we saw that the longer the render time the median power tick increased until we reached a point where the power per tick gets capped at around 110 watts (further investigation found out that this was due to the GPU utilization being maxed out/ limited).

Doing this produced allot of graphs all with the same pattern, all had what seems like await period of around 7ish second where power was low before power and utilization jumped high. This suggests that the rendering needs about 7 seconds to perform some task/calculation, no matter what it is going to render.

Seeing that as render time typical power used increased I plotted the following.

```{r gpu_render_highlight}

#gpu_render_highlight_table

ggplot(gpu_render_highlight_table, aes(render_time, median)) + geom_line() + geom_point() + geom_errorbar(aes(ymin=qt1, ymax=qt3), width=0.25) +  ggtitle("Median Power usage per tick  by Render time") + ylab("Median Power usage per tick in Watts") +  xlab("Render time (Sec)")+ labs(caption = "A tick is 2 seconds|Error bars are spread around point")  

#ggplot(gpu_render_highlight_table, aes(render_time, mean)) + geom_line() + geom_point() + geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=0.25)
```
We see that once the time needed to render a tile reaches around 50 seconds we see time to render stabilizes at around 110 watts per seconds. 

Finally, Plotting the amount of power used in a render task we see that as render time increases the amount power increases in an almost linear fashion 

```{r gpu_render_highlight_sum}

ggplot(gpu_render_highlight_sum, aes(render_time, median)) + geom_line() + geom_point() + geom_errorbar(aes(ymin=qt1, ymax=qt3), width=0.25) +  ggtitle("Power used by render time") + ylab("Median total power usage in Watts") +  xlab("Render time")  + labs(caption = "Error bars are spread around point")  


#ggplot(gpu_render_highlight_sum, aes(render_time, mean)) + geom_line() + geom_point() + geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=0.25)
```

### Can we quantify the variation in computation requirements for particular tiles?

For a more detailed breakdown please see GPU_Power_Usage report 
 
Using the coordinates tied to each task I was able to create a heat map of how much power was typically used in the rendering of each tile. I also created a heat map of how long it took to render each tile.

```{r gpu_renders_xy_power}

 ggplot(gpu_renders_xy_table %>% filter(level != 4) , aes(y, x, fill= median)) + geom_tile()+ facet_wrap(~ level,scales = "free")+ scale_fill_gradient(low="grey", high="blue") + ggtitle(" Power used in the rendering of the tile by job")+ scale_y_reverse() + theme(legend.position="bottom") + labs(fill = "Power Watts") + labs(caption = "Cords switched to match image produced")

```

```{r gpu_renders_xy_time}
 ggplot(render_with_cords %>% filter(level != 4) , aes(y, x, fill= diff)) + 
 geom_tile() + facet_wrap(~ level,scales = "free") + scale_fill_gradient(low="grey", high="blue")  + ggtitle("Time needed to render each tile by job") + scale_y_reverse() + theme(legend.position="bottom") + labs(fill = "Time seconds") + labs(caption = "Cords switched to match image produced")
```

Looking at the heatmap, tiles that seem to have rendered a large flat area like a grassy patch/pond used the least amount of energy. Roads and roofs that use the mid-amount of power during render. It seems like the tiles that use the most power to render are tiles that contain trees. We also see that this pattern also exists in the Render time heat map suggesting that what is being rendered affects how long a tile takes to render and because of that it also increases the amount of power used to render that tile.

This suggests that different tiles will require different computation requirements.

### Is whats is actually being rendered effect how long it takes to render the tile.

A lot of my previous analysis showed me that what is keeping the Terapixel image back is the amount of time it takes to render a tile. Long render times increase power used, heat and GPU utilisation. I wanted to find out if what we are rendering in each tile effects the rendering time. I felt this was important as if we could work out what objects where causing an increase in render time then work could be done to change how the render deals with these object hopefully saving time.

Originally, I created a way that allowed me to split an image of the full Terapixel image into the tiles that were being rendered. This allowed me to see what was in certain tiles as I could filter tiles by render times and get the related selection of the map back. However, I felt I could create a better way. I ended up using some python code that would allow me to apply a K-means clustering algorithm to an image so that I could perform Colour Quantization. Colour Quantization is the process of reducing the number of Colour in an image, I wanted to do this to make classifying what is in a tile easier. Doing this I was able to split an image of the Terapixel map into 15 distinct colours with each colour potentially relating to a distinct object. Splitting this image into the appropriate number of tiles and then working out what colour was in each pixel of each tile I was able to work out what was in each tile.

Here is that Color Quantization image (K was 15).
```{r K15_map}

plot(K15_map)
```
I will admit the there are a few mistakes with this image. It has grouped the dark shadows and roads together due to the fact they where both really dark hues in the original image. Its has also turned the light blue in the original image (which I believe was flat ground) into roofs but as this blue only appeared in a small part of the graph I don't think It would cause any issues.

After working out what each color meant I was able to determine the most dominate object in a tile.
```{r colours}

ggplot(job_8_Tiles_dom, aes(y, x, fill= dominate)) + 
 geom_tile() + ggtitle("What is the dominate object in an tile?") + scale_y_reverse() + labs(caption = "Cords switched to match image produced")

```
Looking at the majority colour in a tile we see that we have clumps of grass, dirt and roofs in a sea of primary tiles made up of roads/shadows. We also notice that out of 15 objects only 10 are dominated in a title and some only dominate in one suggest that the colours can be split into two groups.

- Used to represent something, i.e water and roofs
- used to represent details or shadows.

My next idea was to work out the correlation between each of the different objects and the render time. By doing this It would distinctly show if and how each of the different objects affected the amount of time needed to render each tile. The following is that correlation plot.

```{r colours_Corr}
task_join_col <- inner_join(render_with_cords %>% filter(level == 8) , job_8_Tiles_dom , by = c("x","y"))

task_join_col <- task_join_col %>% rename( RENDER_TIME = diff)

ggcorrplot(cor(task_join_col %>% select(RENDER_TIME, roof,roof_details,Roof_Slant_Away_Sun,Grass,Dirt,Inside_of_buildings,Orange,Roof_Slant_Face_Sun,Stone_ground,Key,extra_shadows,tree,light_shadow,DeepShadow_Roads,water))
, hc.order = TRUE, type = "lower",  insig = "blank",
     outline.col = "white") + ggtitle("Correrlation between object types and render time")

```
We see that the more of flat objects that are not effected shadows and are fully in daylight (so Stone_ground, insides of building, the Key, grass and dirt) that appear in a tile then the quicker the that tile is to render.

We also see that the more that shadows appear (so Deep shadows, light shadows and extra shadows) in the tile then the longer that the tile takes to render (with quite a strong correlation).

Anything to do with roofs has caused an increase in render time.

Other bits of information that we can take away from this is that 

Trees seem to one of the main reasons for the shadows

Extra shadows seem to effect mainly roofs.

Overall this tell us that what is being rendered does have an effect on the render time and that some objects types (mainly those used in details) have a greater impact on the render time.

### GPU errors

We have a lot of GPUs that have been used to generate these images. The generation of this image depends on all theses GPUS working together. If a GPU is having any issues then we will need to repair/replace them as a single issue may slow down the overall creation of a Terapixel image.

Originally I wanted to use the fact that the median time to complete a task was 43 seconds. This didn't work as I wanted it, there are way too many additional factors that affect how long it takes to complete a task. So saying "oh this GPU must have flaws because took on average a few seconds longer to complete each it's of its task" didn't feel a good way of highlighting errors. It's possible that a GPU with no errors was just given a bunch of tasks that just took a while to processes.

I instead decided to look at the median amount of time between each task on the GPU (so the time between the GPU last task ending and its next task starting). I feel like this would be a better way to gauge which GPUs may have errors as its something that cant be affected by the rendering processes. We find that the median amount of time between each task is 2.4 seconds.

Creating a bar chart of the time typical time between GPU task we get the following.
```{r wait}

idle_time <- link_gpu_task %>% group_by(gpuSerial) %>% mutate(idletime = lagtime - lag(timestamp.x)) %>%  na.omit(idletime) %>% ungroup()


ggplot(data=idle_time %>% group_by(gpuSerial) %>% summarize(median =  round(median(idletime), 1)) %>% count(median) , aes(x=median, y=n)) +
  geom_bar(stat="identity") + ggtitle("Count of median time between GPU tasks") + ylab("Count of GPU") + xlab("Time (seconds)")
```
Looking at this we see that we have around 20 GPUS that median time between tasks is 2.8 seconds or longer. I suggest that further investigation should be done into these GPUS as the fact that they are taken longer to start new tasks suggests that they may be some issues with them.

### GPU errors - The upload issues

We have a lot of GPUs that have been used to generate these images. The generation of this image depends on all theses GPUS working together. If a GPU is having any issues then we will need to repair/replace them as a single issue may slow down the overall creation of a Terapixel We know that trying to work out which individual GPUs had errors may be difficult so I decided to look into if there where any issues affecting a wide range of GPUS instead.

Mentioned earlier, When we plotted out the x y coordinates and the time is taken to fully complete each task we noticed that there are a few tasks that took a while to complete (almost double the expected amount), especially in the low Y cord in the job 12 graphs.

```{r spefic_cord}

link_gpu_task_xy <- left_join(link_gpu_task, xy_df ,by = c("jobId","taskId") )

ggplot(link_gpu_task_xy %>% filter(level == 12 , y < 20, total_time > 50), aes(y, x, fill= total_time)) + 
  geom_tile() + scale_fill_gradient(low="grey", high="blue") + ggtitle("The band of high time to complete the tile") + scale_y_reverse()  + theme(legend.position="bottom") + labs(fill = "Time taken (seconds)")
```
Doing some further investigation into this I found out that this was due to the upload times for these tasks being extremely high. 

```{r upload_timeline}

y_lessthan_15 <- link_gpu_task_xy %>% filter(level == 12 , y < 15, total_time > 50)

Over50 <- y_lessthan_15$taskId

ggplot(subset(times %>% na.omit(times) %>% filter(jobId == "1024-lvl12-7e026be3-5fd0-48ee-b7d1-abd61f747705", eventName == "Uploading"),taskId %in% Over50) 
, aes(x=timestamp, y=diff)) +
  geom_point(size=2, shape=23)  + ggtitle("Upload times for the highlighted tasks")+ ylab("Upload time") + xlab("Time stamp")

```
These high upload times seemed to only really appear between 07:41:00 and 07:44:00. It also seemed to effect allot more tasks than the ones highlighted. This "event" only occurs between this time suggesting that something happened at this point which caused the the uploads to increase by roughly 20 fold. Originally I though that this might be caused by the GPUS switching from doing task from job 8 to doing tasks from job 12 but this dost seem to be the case. I honestly couldn't figure out what caused this so more investigation should be done on this in order to find any errors. These aren't the first task to be completed, we are a little bit into the processes suggested that might not be a starting issue.


## The success and weaknesses of my project

As one of the goals was to find potential ways to reduce the amount of time taken to produce a Terapixel image, I feel like I have been very successful in completing this task. This was important as the less time needed to create an image then the less money needed to rent the IaaS. Throughout my analysis, I have discovered that the event in which most of the improvements can be made is the rendering task. Rendering takes up the most of the time needed to complete each task so and we see that as the correlation between the time needed to render an image and increases in power draw per tick, heat per tick and GPU utilization. I feel like my investigation into these areas was successfully and methods that I used (mainly comparisons of median) allowed me to get evidence that shows me a clear picture of what was going on and allowed me to answer the question I had asked.

I also went quite in-depth with my analysis meaning I was able to find that there is some sort of wait period at the start of each rendering tasks which might be slowing down the whole processes. This could be important as finding out if this wait is needed or not could take almost 7 seconds off each event run, If this is possible then this could be massive. 

I feel like my investigation into if whats in a tile effects render time is a massive success as the evidence I received from that answered the question and could be beneficial to future works. Using a K-means clustering algorithm to Color Quantization I was able to work out what exactly was causing some tiles to have long render times via correlation. The correlation graph clearly shows that more shadows and details in an tile the longer to it takes to render. I feel like this is very important in achieving my goals as it tells us what exactly is causing rendering to take so much time. However, there are a few weakness with this. Firstly the image I used to do this was a screenshot of the Terapixel so its not the same as the real image (screenshot has a lot fewer pixels than real image). This means the image I used to perform Color Quantization is not 100% perfect as it has compressed some details down. Secondly, the K I used was somewhat random. I chose a K of 15 so that my process would get 15 get different objects(colours). This may not be the perfect number however it did give me enough to work with. To little K and I wouldn't have enough to work with (objects would get blend together) to Big of a K and I would have too much to work with (meaning that It wouldn't be able to classify objects easily). Another issue was that I was only able to look into the tiles in job level 8. This is because my PC had an issue trying to split the image into the 65536 tiles needed for job level 12. If I had better hardware I would like to try this.

I do feel like my project has a few weaknesses. I found it difficult trying to find GPUS that where problematic. Originally I wanted to check the median time it took each GPU to complete each rendering tasks against one another if we could work out GPUs that took more time to complete the task on average time we might be able to find errors. However, I had a lot of difficulty with this. We know that the rendering event makes up most of the time in a task so its possible that a GPU that is perfectly fine but just had a few tiles that took a while would be labelled as defective. Whilst I did find some GPUs that were affected by a long time between jobs, I don't think I answered this question too well. I could only find an event that effected most GPUs once (which is important to look into) but couldn't lable out specific GPU as having errors.   

Another weakness of this project is that it was based around only one run of the creation of the Terapixel image. It would be interesting to see what would change/hold if we ran another investigation into another Terapixel image.

## Future implications for work from this project 

From my project, I believe that work should be undertaken on reviewing the rendering processes. We have seen that rendering takes up the majority of the time to complete a tile. We have seen that the more detailed a tile is the longer the render time, we also see that areas that contain shadows/shade seem to increase the render time and that areas with a high density of trees seem to cause an issue.

However, as the whole point of the Terapixel image is to be detailed removing detail to decrease the render time would be against the whole point of this project.

Instead, I believe that work should be done on the shadows/shading processes. It seems that all the tiles that visibly contain something casting a shadow or being in the shade cause the render time to increase. This might be due to the algorithm needed to create the shadows having to spend calculating how the shadow should fall onto the ground and how it should interact with whatever it touched. 

The fact that the shadows are causing an increase in render time tells us if we rendered an image where the sun was at sunset position (so causing longer shadows) then this would cause an increase in render time. On the flip side of this, it suggests that if the "sun" was positioned at midday (so in the centre of the graph) then we should have shorter render time.

Even if the algorithm needs to calculate shadows can be improved by a second or two then, in the long run, it would shave off a few minutes processing the fully rendered image. 

If we are unable to improve the rendering of the Terapixel image its possible that we could use this information to try and predict how long it would take to render a new image. This could then be used to calculate how much power would be used in generating the image meaning that you could work out the cost of the image generating the image / renting IaaS resources.

An investigation should be launched into what the GPU is doing at the start of each rendering Processes. We see that is that it seems to be a 7 second period at the start of every rendering task where it's doing something that's not increasing the power drawn in (so I don't think it's rendering anything). If this is an error then its possible we could shave an extra 7ish seconds off each rendering tasks. 

Finally, a further investigation should be launch into what happened between 07:41:00 and 07:44:00 that caused such a spike in upload times. As this increase in upload time caused some tasks to take twice as long as expected. We need to figure out what had happened and why to prevent this from happening in the future as it slows down efficiency.

## Reflection

I enjoyed the project, The topic was really interesting and the data that we were given allowed me to try and answer so many different questions. This meant I could try a lot of different methods however I felt like I stuck onto using comparison via medians a lot as I felt that was the best method to use to answer the question I was asking. There were 1024 different GPUS doing so many different things that trying to the analysis they could be a bit overwhelming at times. I feel like going forward I should try and branch into different methods more to make my report more visually interesting and it may help me discover new things.

Another issue I had was the I feel like I may have gone a bit overboard in this project and tried to answer a lot of questions. Whilst this helped me complete my goals, looking back through my project I think It would have been better if instead of investigating a new question I went back through what I had previously done and investigate further to try to uncover some more specific insights / refine my investigation. I think in the future I will try and narrow down my scope so that I focus on one section more to uncover a better answer to a question instead of trying to briefly skim over a bunch of question. This would help make a more detailed report in which I can be more confident in my answers whilst also making the report easier to write.

What I found the most difficult was trying to investigate which GPU was potentially defective, I feel like this was down to a mixture of my methodology is wrong and there are so many "moving parts" in this data that could affect results that would have made tracking down issues difficult. I feel like I need to investigate more strategies to deal with an issue like this in the future. 

I am really happy with the work I did on the K means colour Quantization as I feel like it adds a lot to the project. I had the idea of working out if what was in each tile near the start of the project so I'm really happy its something that I ended up touching. Learning about colour quantization was actually really fun to do and investigate. Know that I sort of understanding how it works I'm thinking of ways to use it in the future for something.

One issue I had was trying to find improvements to the actual technology/cloud structure used in this report. Whilst I read the report that this project was and got a grip with how it works it was really difficult to get an understanding of how it could be improved. Clearly, a lot of effort has gone into the methods used to create the Terapixel image and I feel like I don't have the know-how to suggest improvements with the tech used.  

I normally find writing reports difficult but this one came pretty straight forward for me which was nice. I knew what I wanted to say and how I would structure it.
Also, I believe I'm starting to really get a hang of R projects, it makes doing a project like this really easy to make reproducible.